{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kaffe.tensorflow import Network\n",
    "\n",
    "class troNet(Network):\n",
    "    def setup(self):\n",
    "        (self.feed('data')\n",
    "             .conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "             .max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "             .lrn(2, 2e-05, 0.75, name='norm1')\n",
    "             .conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "             .lrn(2, 2e-05, 0.75, name='norm2')\n",
    "             .conv(3, 3, 384, 1, 1, name='conv3')\n",
    "             .conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "             .conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "             .conv(3, 3, 256, 1, 1, group=2, name='conv6')\n",
    "             .max_pool(3, 3, 2, 2, padding='VALID', name='pool6')\n",
    "             .fc(4096, name='fc7_new')\n",
    "             .fc(2543, relu=False, name='fc8_new')\n",
    "             .softmax(name='prob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from amosnet import troNet\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_input_data = np.load(\"data1.npy\")\n",
    "\n",
    "\n",
    "images = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "weight_file = 'data1.npy'\n",
    "img = imread('image-00324.png')\n",
    "\n",
    "x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "\n",
    "model = troNet({'data': x})\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model.load(weights_file, sess)\n",
    "     \n",
    "    batch = img.reshape((1,227,227,3))\n",
    "    \n",
    "    output = sess.run(model.get_output(), feed_dict={x: batch}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image-00324.png [[[  0.           0.           1.72047436 ...,   0.           0.\n",
      "     4.72068167]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  4.00791931   0.           4.64759874 ...,   0.           0.           0.        ]]\n",
      "\n",
      " [[ 15.61711121   0.           0.29853916 ...,   0.           0.\n",
      "     4.72068167]\n",
      "  [ 10.43680859   0.           3.6337781  ...,   0.           0.           0.        ]\n",
      "  [ 15.90464592   0.           3.6337781  ...,   0.           0.28630865\n",
      "     2.32373238]\n",
      "  [ 15.90464592   0.           0.76783544 ...,   0.           0.\n",
      "     2.32373238]\n",
      "  [ 10.19052792   0.           0.         ...,   4.89248562   0.           0.        ]\n",
      "  [ 15.43102837   0.           3.54442453 ...,   3.21233511   0.           0.        ]]\n",
      "\n",
      " [[ 20.37113571   0.           0.         ...,   0.           0.           0.        ]\n",
      "  [ 14.18690491   0.           1.49672484 ...,   0.           0.           0.        ]\n",
      "  [ 15.90464592   0.           1.49672484 ...,   0.           0.           0.        ]\n",
      "  [ 15.90464592   0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  8.72711182   0.           0.         ...,   4.89248562   0.           0.        ]\n",
      "  [  4.85096931   0.           3.54442453 ...,   3.31987977   2.70674419\n",
      "     0.        ]]\n",
      "\n",
      " [[ 10.54965115   0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  2.86656332   0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           1.45945215 ...,   0.           4.25345421\n",
      "     0.        ]]\n",
      "\n",
      " [[  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           5.82927895 ...,   0.           0.\n",
      "     0.93605679]\n",
      "  [  0.           0.          15.37741852 ...,   0.           0.           0.        ]\n",
      "  [  0.           0.          17.6352272  ...,   0.           0.           0.        ]\n",
      "  [  0.           0.          15.32218075 ...,   0.           0.           0.        ]\n",
      "  [  0.           0.          12.6094017  ...,   0.           0.           0.        ]\n",
      "  [  0.           0.          16.97637558 ...,   0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# %load infer.py\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "def load_weight(sess, weight_path ):\n",
    "    pre_trained_weights = np.load(open(weight_path, \"rb\"), encoding=\"latin1\").item()\n",
    "    keys = sorted(pre_trained_weights.keys())\n",
    "    for k in keys:\n",
    "    #for k in list(filter(lambda x: 'conv' in x,keys)):\n",
    "        with tf.variable_scope(k, reuse=True):\n",
    "            temp = tf.get_variable('weights')\n",
    "            sess.run(temp.assign(pre_trained_weights[k]['weights']))\n",
    "        with tf.variable_scope(k, reuse=True):\n",
    "            temp = tf.get_variable('biases')\n",
    "            sess.run(temp.assign(pre_trained_weights[k]['biases']))\n",
    "\n",
    "def conv(input, filter_size, in_channels, out_channels, name, strides, padding, groups):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        filt = tf.get_variable('weights', shape=[filter_size, filter_size, int(in_channels/groups), out_channels])\n",
    "        bias = tf.get_variable('biases',  shape=[out_channels])\n",
    "    if groups == 1:\n",
    "        return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input, filt, strides=strides, padding=padding), bias))\n",
    "    else:\n",
    "        # Split input and weights and convolve them separately\n",
    "        input_groups = tf.split(split_dim = 3, num_split=groups, value=input)\n",
    "        filt_groups = tf.split(split_dim = 3, num_split=groups, value=filt)\n",
    "        output_groups = [ tf.nn.conv2d( i, k, strides = strides, padding = padding) for i,k in zip(input_groups, filt_groups)]\n",
    "\n",
    "        conv = tf.concat(concat_dim = 3, values = output_groups)\n",
    "        return tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "\n",
    "def fc(input, in_channels, out_channels, name, relu):\n",
    "    input = tf.reshape(input , [-1, in_channels])\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        filt = tf.get_variable('weights', shape=[in_channels , out_channels])\n",
    "        bias = tf.get_variable('biases',  shape=[out_channels])\n",
    "    if relu:\n",
    "        return tf.nn.relu(tf.nn.bias_add(tf.matmul(input, filt), bias))\n",
    "    else:\n",
    "        return tf.nn.bias_add(tf.matmul(input, filt), bias)\n",
    "\n",
    "\n",
    "def pool(input, padding, name):\n",
    "    return tf.nn.max_pool(input, ksize=[1,3,3,1], strides=[1,2,2,1], padding=padding, name= name)\n",
    "\n",
    "\n",
    "#placeholders\n",
    "x = tf.placeholder(tf.float32, shape = [None, None, None,3])\n",
    "\n",
    "#AmosNet Conv-Layers\n",
    "net_layers={}\n",
    "net_layers['conv1'] = conv(x, 11, 3, 96, name= 'conv1', strides=[1,4,4,1] ,padding='VALID', groups=1)\n",
    "net_layers['pool1'] = pool(net_layers['conv1'], padding='VALID', name='pool1')\n",
    "net_layers['lrn1']  = tf.nn.lrn(net_layers['pool1'], depth_radius=2, alpha=2e-5, beta=0.75,name='norm1')\n",
    "\n",
    "net_layers['conv2'] = conv(net_layers['lrn1'], 5, 96, 256, name= 'conv2', strides=[1,1,1,1] ,padding='SAME', groups=2)\n",
    "net_layers['pool2'] = pool(net_layers['conv2'], padding='VALID', name='pool2')\n",
    "net_layers['lrn2']  = tf.nn.lrn(net_layers['pool2'], depth_radius=2, alpha=2e-5, beta=0.75,name='norm2')\n",
    "\n",
    "net_layers['conv3'] = conv(net_layers['lrn2'], 3, 256, 384, name='conv3', strides=[1,1,1,1] ,padding='SAME', groups=1)\n",
    "\n",
    "net_layers['conv4'] = conv(net_layers['conv3'], 3, 384, 384, name='conv4', strides=[1,1,1,1] ,padding='SAME', groups=2)\n",
    "\n",
    "net_layers['conv5'] = conv(net_layers['conv4'], 3, 384, 256, name='conv5', strides=[1,1,1,1] ,padding='SAME', groups=2)\n",
    "\n",
    "net_layers['conv6'] = conv(net_layers['conv5'], 3, 256, 256, name='conv6', strides=[1,1,1,1] ,padding='SAME', groups=2)\n",
    "net_layers['pool6'] = pool(net_layers['conv6'], padding='VALID', name='pool6')\n",
    "\n",
    "net_layers['fc7'] = fc(net_layers['pool6'],  6*6*256, 4096, name='fc7_new', relu = 1)\n",
    "net_layers['fc8'] = fc(net_layers['fc7'], 4096, 2543, name='fc8_new', relu = 0)\n",
    "\n",
    "net_layers['prob'] = tf.nn.softmax(net_layers['fc8'])\n",
    "net_layers['pred'] = tf.argmax(tf.nn.softmax(net_layers['fc8']), dimension = 1)\n",
    "\n",
    "def normalize_input(img):\n",
    "    img = img.astype(dtype=np.float32)\n",
    "    img = img[:, :, [2, 1, 0]] # swap channel from RGB to BGR\n",
    "    img = img - [104,117,124]\n",
    "    return img\n",
    "\n",
    "def load_preprocess_image(img_path):\n",
    "    img = misc.imread(img_path)\n",
    "    img = normalize_input(img)\n",
    "    img = misc.imresize(np.asarray(img), (256, 256))\n",
    "    return img\n",
    "\n",
    "def main(weight_path, img_paths, layer):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        load_weight(sess, weight_path=weight_path)\n",
    "\n",
    "        with open('results.csv',\"w\") as f:\n",
    "            writer = csv.writer(f, delimiter=',',  quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "            for img_path in img_paths:\n",
    "                img = load_preprocess_image(img_path)\n",
    "\n",
    "                img = tf.slice(img, begin=[14, 14, 0], size=[227, 227, -1])\n",
    "                img = tf.expand_dims(img, dim=0)\n",
    "                [img] = sess.run([img])\n",
    "\n",
    "                [result] = sess.run(net_layers[layer], feed_dict={x:img})\n",
    "\n",
    "                print(img_path, result)\n",
    "                result = result.flatten()\n",
    "\n",
    "                writer.writerow(result)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--layer\", help=\"layer to output eg:conv1, pool2, fc7\", type=str)\n",
    "#     parser.add_argument( \"--weight_path\", help=\"path to stored CNN-weights\",  type=str)\n",
    "#     parser.add_argument( \"--img_path\", nargs='+', help=\"path of the image\",  type=str)\n",
    "\n",
    "#     config = parser.parse_args()\n",
    "#     print(config)\n",
    "    main('./data1.npy', ['image-00324.png'],'pool6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('results.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        a=row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tushar/Heavy_Datasets/gta_data/final/train.txt', 'r') as file1:\n",
    "    train_data=[]\n",
    "    for row in file1:\n",
    "        train_data.append(row.split())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders= train_data[:4]\n",
    "folders = folders[:2]\n",
    "# folders = [i[1] for i in folders]\n",
    "train_data = train_data[4:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F5': 'city_2_night_return', 'F6': 'city_2_sunnyrain_returntoairport'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders_dir = {}\n",
    "for i in folders:\n",
    "    folders_dir[i[0]]=i[1]\n",
    "folders_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [train_data[i+1:i + 3] for i in xrange(0, len(train_data), 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/home/tushar/Heavy_Datasets/gta_data/final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['F6',\n",
       "  '183',\n",
       "  '184',\n",
       "  '185',\n",
       "  '186',\n",
       "  '187',\n",
       "  '188',\n",
       "  '189',\n",
       "  '190',\n",
       "  '191',\n",
       "  '192',\n",
       "  '193',\n",
       "  '194',\n",
       "  '195',\n",
       "  '196',\n",
       "  '197',\n",
       "  '198',\n",
       "  '199',\n",
       "  '200',\n",
       "  '201',\n",
       "  '202'],\n",
       " ['F5',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
